{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Текст:\n",
    "\n",
    "\"...На краю дороги стоял дуб. Он был, вероятно, в десять раз старше берез, составлявших лес, в десять раз толще и в два раза выше каждой березы. Это был огромный, в два обхвата дуб, с обломанными суками и корой, заросшей старыми болячками. С огромными, неуклюже, несимметрично растопыренными корявыми руками и пальцами, он старым, сердитым и презрительным уродом стоял между улыбающимися березами. Только он один не хотел подчиниться обаянию весны и не хотел видеть ни весны, ни солнца.\n",
    "\n",
    "Этот дуб как будто говорил: «Весна, и любовь, и счастье! И как не надоест вам все один и тот же глупый, бессмысленный обман! Все одно и то же, и все обман! Нет ни весны, ни солнца, ни счастья. Вон смотрите, сидят задавленные мертвые ели, всегда одинокие, и вон я растопырил свои обломанные, ободранные пальцы, выросшие из спины, из боков — где попало. Как выросли — так и стою, и не верю вашим надеждам и обманам».\n",
    "\n",
    "Князь Андрей несколько раз оглянулся на этот дуб, проезжая по лесу. Цветы и трава были и под дубом, но он все так же, хмурый, неподвижный, уродливый и упорный, стоял посреди них.\n",
    "\n",
    "«Да, он прав, тысячу раз прав этот дуб, — думал князь Андрей. — Пускай другие, молодые, вновь поддаются на этот обман, а мы знаем: наша жизнь кончена!» Целый ряд мыслей, безнадежных, но грустно-приятных, в связи с этим дубом возник в душе князя Андрея. Во время этого путешествия он как будто вновь обдумал всю свою жизнь и пришел к тому же успокоительному и безнадежному заключению, что ему начинать ничего было не надо, что он должен доживать свою жизнь, не делая зла, не тревожась и ничего не желая...\n",
    "\n",
    "Уже было начало июня, когда князь Андрей, возвращаясь домой, въехал опять в ту березовую рощу, в которой этот старый, корявый дуб так странно и памятно поразил его. «Здесь, в этом лесу, был этот дуб, с которым мы были согласны. Да где он?» — подумал князь Андрей, глядя на левую сторону дороги. Сам того не зная, он любовался тем дубом, которого искал, но теперь не узнавал его.\n",
    "\n",
    "Старый дуб, весь преображенный, раскинувшись шатром сочной, темной зелени, млел, чуть колыхаясь в лучах вечернего солнца. Ни корявых пальцев, ни болячек, ни старого горя и недоверия — ничего не было видно. Сквозь столетнюю жесткую кору пробивались без сучков сочные, молодые листья, так что верить нельзя было, что это старик произвел их. «Да это тот самый дуб», — подумал князь Андрей, и на него вдруг нашло беспричинное весеннее чувство радости и обновления. Все лучшие минуты его жизни вдруг в одно и то же время вспомнились ему. И Аустерлиц с высоким небом, и Пьер на пароме, и девочка, взволнованная красотою ночи, и эта ночь, и луна — все это вдруг вспомнилось ему.\n",
    "\n",
    "«Нет, жизнь не кончена в тридцать один год, — вдруг окончательно и бесповоротно решил князь Андрей. — Мало того, что я знаю все то, что есть во мне, надо, чтобы и все знали это: и Пьер, и эта девочка, которая хотела улететь в небо. Надо, чтобы не для одного меня шла моя жизнь, чтобы на всех она отражалась и чтобы все они жили со мной вместе»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Статистический анализ текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет числа слов в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов  639\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import Text\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "document = open(\"D:\\downloads\\wp.txt\",encoding='utf8').read()\n",
    "tokens = word_tokenize(document.lower())\n",
    "text = Text(tokens)\n",
    "print('Всего слов ',len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет числа слов в тексте, не учитывая знаки препинания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего слов  501\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "no_punc=tokenizer.tokenize(document.lower())\n",
    "text = Text(no_punc)\n",
    "print('Всего слов ',len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчет числа предложений в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений:  28\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences=sent_tokenize(document.lower())\n",
    "print('Всего предложений: ',len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее часто встречающиеся слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 83),\n",
       " ('и', 36),\n",
       " ('.', 25),\n",
       " ('в', 13),\n",
       " ('не', 13),\n",
       " ('—', 10),\n",
       " ('дуб', 9),\n",
       " ('он', 9),\n",
       " ('все', 9),\n",
       " ('ни', 8)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "tokens = word_tokenize(document.lower())\n",
    "text = Text(tokens)\n",
    "fdist = FreqDist(text)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее часто встречающиеся слова (без знаков препинания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 36),\n",
       " ('в', 13),\n",
       " ('не', 13),\n",
       " ('дуб', 9),\n",
       " ('он', 9),\n",
       " ('все', 9),\n",
       " ('ни', 8),\n",
       " ('на', 7),\n",
       " ('этот', 6),\n",
       " ('князь', 6)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "no_punc=tokenizer.tokenize(document.lower())\n",
    "text = Text(no_punc)\n",
    "fdist = FreqDist(text)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исключим предлоги и прочие стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('дуб', 12),\n",
       " ('весь', 7),\n",
       " ('князь', 7),\n",
       " ('андрей', 7),\n",
       " ('старый', 6),\n",
       " ('жизнь', 6),\n",
       " ('стоять', 4),\n",
       " ('весна', 4),\n",
       " ('обман', 4),\n",
       " ('знать', 4)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Создаем лемматизатор и список стоп-слов\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "russian_stopwords.extend(['что', 'это', 'так', 'вот', 'быть', 'как', 'в', '—', 'к', 'на'])\n",
    "\n",
    "\n",
    "def no_punkt(document):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    a = tokenizer.tokenize(document.lower())\n",
    "    res = \" \".join(a)\n",
    "    return res\n",
    "\n",
    "def preprocess(document):  \n",
    "    a=no_punkt(document)\n",
    "    tokens = mystem.lemmatize(a)\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    text = \" \".join(tokens)\n",
    "    return text\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "nt=preprocess(document)\n",
    "tokens = word_tokenize(nt)\n",
    "text = Text(tokens)\n",
    "fdist = FreqDist(text)\n",
    "fdist.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Лексикографический анализ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "no_punc=tokenizer.tokenize(document.lower())\n",
    "\n",
    "a=pos_tag(no_punc, lang='rus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наиболее часто встречающиеся части речи в тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\n"
     ]
    }
   ],
   "source": [
    "from statistics import mode\n",
    "\n",
    "l=[]\n",
    "for x in a:\n",
    "    l.append(x[-1])\n",
    "    \n",
    "print(mode(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
